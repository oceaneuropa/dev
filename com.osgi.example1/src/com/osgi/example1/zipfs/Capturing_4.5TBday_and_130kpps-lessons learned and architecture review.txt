https://groups.google.com/forum/#!topic/moloch-fpc/NYHkR8VQF_g

We have been running Moloch in production for a year as of last month, and I wanted to share some lessons learned and had a few questions about how we can further improve our stability and scale our system.

Our architecture is as follows:

3x Dell rack servers each with:
* 384GB of RAM
* (2x) 6-core, hyper-threaded Intel Xeons
* 15k spinning disks for OS
* 10k spinning RAID-5 for bulk VM storage
* Ubuntu 14.04 LTS (Kernel 3.13 - next iteration will be >3.20)

1 server runs capture and viewer on bare metal but sends PCAP to 10Gb NFS storage (see below)
2 other servers run 4x Elasticsearch VMs each with local EXT4 storage passed through to KVM VMs

The NFS is 10GbE to a BackBlaze v4.5 with 45x 6TB 7.2k drives and 2x 500GB of SSD caching (BTRFS on RockStor).

Moloch is capturing on a 10GbE interface with libpcap.

Elasticsearch VMs are 4 CPU with 48GB of RAM. Each uses local storage on the VM host through a KVM mapped filesystem. We set a 25GB java heap and have 2 master/router nodes and 6 data nodes. 

We are currently collecting 4-4.5 TB/day and average 130k packets-per-second with bursts of over 400kpps.

I have spent hundreds of hours tuning Elasticsearch indexing, throttling, writing scripts to reallocate shards after a Moloch crashes,and  doing detailed Elasticsearch logging to ELK to trace down java errors, and now with ES 2.2, I feel very confident in our Elasticsearch cluster even at >250kpps.

We tried many types of storage architecture for the PCAP with various different failures. Our platform has been through the following iterations to get to this reasonably stable point:
VM local filesystem using KVM pools backed by local ZFS on Linux RAIDZ-2
The controllers in the R720 didn't support JBOD, so we passed through 10x single-disk RAID-0 arrays and build the zpool out of those
ZFS on Linux was way too slow, 30% slower than FreeBSD
VM local filesystem using KVM pools backed by local ZFS on Linux RAID-1
Again, ZoL was just too slow
VM local filesystem using KVM pools backed by 10gb iSCSI to a RAIDZ-2 ZFS on FreeBSD
ZFS schedulers were too slow with iSCSI on FreeBSD
Exporting the zpool as a block device and formatting it with either XFS or EXT4 didn't help
VM local filesystem using KVM pools backed by the KVM host's local 10k RAID-5
Linux schedulers between the guest and host were too slow. Tried CFQ and noop with lots of tuning of kernel buffers to no avail
Tried XFS and EXT4 on the guest to the pool, but neither was fast enough
VM passthrough filesystem backed by KVM host's local 10k RAID-5 with XFS
XFS didn't seem fast enough. I tuned stripe sizes based on the RAID controller's block size, but I saw no improvement
We didn't have enough storage (10TB) for this to be reasonable for our workload anyway
VM passthrough filesystem backed by KVM host's local 10k RAID-5 with EXT4
EXT4 seemed fast enough, but again we didn't have enough storage for our workload
VM passthrough filesystem backed by 10GbE iSCSI to a RAIDZ-2 ZFS on FreeBSD
ZFS scheduler issues on FreeBSD
iSCSI stability issues with ZFS on FreeBSD, especially if we had to reboot the KVM host
VM passthrough filesystem backed by 10GbE NFS to a RAIDZ-2 ZFS on FreeBSD
NFS was more stable but still not fast enough with RAID-Z2 ZFS on FreeBSD
VM passthrough filesystem backed by 10GbE NFS to a RAID-1 ZFS on on FreeBSD
Still not fast enough, probably again because of the schedulers (spent dozens of hours trying to tune and reset everything several times thinking I overtuned)
VM passthrough filesystem backed by 10GbE NFS to a RAID-1 on RockStor (RHEL 7 with BTRFS)
This system FLIES! 
We've not encountered disk queuing above 1% since moving to this unless we really SLAM the host. Right now i'm importing 120TB of PCAP into the Moloch host while simultaneously indexing from a live Moloch capture doing ~155kpps. The host is stable, although we are starting to see a queue.
With the ZFS system, we probably never had enough RAM (32GB). It was easier to migrate to BTRFS on Linux than replace the mobo, CPU, and RAM of our BackBlaze to support the 256GB+ we needed.



We recently upgraded ES 1.7.4 and Moloch 0.12 to ES 2.2 and Moloch 0.12.3. I followed all the upgrade guides but ended up corrupting my files and fields indices somehow. Our data wasn't extremely critical since we also have an enterprise product doing full capture alongside Moloch. Before the upgrade, we had over 1.5 trillion sessions for 18 days of capture.



The db.pl script's upgrade command reported that we were on version 25 moving to 26, failed the run with "fields index already exists", then reported that we were on version 0 in subsequent attempts to upgrade and wipe. I eventually exported the users and did an init to get back up and running. We upgraded Moloch to 0.12.3 first then ES to 2.2 after running a db.pl upgrade on ES 1.7.4. I should still have those logs in ELK if they may help resolve a bug.



At this point, we are still occasionally getting drops (anywhere from 0.12% - 13%) during traffic spikes, and I'm trying to figure out how to handle that burst. The logs haven't given any clear indication as to why packets are dropping, and they're not ifdrops, ES queue, or disk queue. Do BPF filters in the config count towards drops?



The hosts' and VMs' resource usage is mild. We don't saturate cores, set Moloch affinity, and have tons of free RAM and Java heap on the ES hosts. 



I followed the steps in the FAQ to try to ensure sane minimums and maximums, although i'm not entirely sure on parameters like pcapBufferSize and dbBulkSize. I've just been tuning numbers until drops bottom out, but it seems in constant flux. Some days it's fine, other days we see a lunchtime burst of traffic and drop >10%.



Here's our config:



[default]
elasticsearch=<snip>:9200,<snip>:9200
passwordSecret = <snip>
interface=p3p1 ## 10GbE Intel NIC
freeSpaceG = 2000
userNameHeader=<snip>
webBasePath = /
viewPort = 8005
viewHost = <snipped IP address>
rotateIndex=daily
certFile=/data/moloch/etc/moloch.crt
keyFile=/data/moloch/etc/moloch.key
httpRealm = Moloch
bpf=not ( portrange 9200-9300 or (src host <snip> or <snip> or <snip> or <snip>) or (dst host <snip> or <snip>) or port <snip> or port <snip> or (src net <snip> or <snip> or <snip> or <snip> or <snip> or <snip> or <snip> or <snip> or <snip> or <snip>) )
pcapDir = /moloch/pcap
maxFileSizeG = 12
tcpTimeout = 600
tcpSaveTimeout = 720
udpTimeout = 30
icmpTimeout = 10
maxStreams = 1000000
maxPackets = 100000
geoipFile = /data/moloch/etc/GeoIP.dat
geoipASNFile = /data/moloch/etc/GeoIPASNum.dat
rirFile = /data/moloch/etc/ipv4-address-space.csv
dropUser=<snip>

dropGroup=<snip>

parseSMTP=true
parseSMB=true
parseQSValue=false
smtpIpHeaders=X-Originating-IP:;X-Barracuda-Apparent-Source-IP:
parsersDir=/data/moloch/capture/parsers
pluginsDir=/data/moloch/capture/plugins
spiDataMaxIndices=2
pcapWriteMethod=thread-direct
pcapWriteSize = 8388608 ## 2048 x 4096
pcapBufferSize = 30000000
dbBulkSize = 10000000
dbFlushTimeout = 5
compressES = false
maxESConns = 100
maxESRequests = 5000
packetsPerPoll = 100000
antiSynDrop = false
logEveryXPackets = 1000000
logUnknownProtocols = false
logESRequests = false
logFileCreation = false





As we are looking to expand our deployment, I was wondering if anyone tiers storage for Moloch. I wanted to propose a large direct attached SSD or FusionIO tier on the capture box in our next revision, but I am unsure how to properly rotate out the PCAP to the storage appliance while still having Moloch able to search from it alongside the local storage tier. My understanding is that multiple directories specified in the config file will round-robin the PCAP files to the targets. We plan to expand our ES cluster to be able to index at least 2 months, and it would be nice to be able to tier something like ~3 days on SSD, ~1 month on the 45- spindle array, and 2 months on another bulk array that's a little slower but won't be indexed from live data. Is there maybe a roadmap to support file rotation like that?



Have we just outgrown a single capture system and need to setup multiple discrete capture boxes feeding one ES each with their own storage back-ends?



Thank you for your input, and I hope this information may help anyone else dealing with storage speed woes.



-Kevin Dougherty